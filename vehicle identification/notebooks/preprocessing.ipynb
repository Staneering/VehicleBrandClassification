{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52160f1-d7ec-4729-8c91-d8604206f741",
   "metadata": {},
   "source": [
    "## **Data Preprocessing (HOG, HSV and LBP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9464ada-b75e-4393-ba6b-17ed4c68346a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "293a3127-3906-4259-83ff-492edfe05bc5",
   "metadata": {},
   "source": [
    "## **Setup, import and paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b29e6a-4daa-469b-96fe-9c3f38e46b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready! VEHICLE_DIR = /home/ostan98/gitfolder/Vehicle_Detection_and_Classification/Data/vehicle\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 1: Setup ────────────────────────────────────────────────\n",
    "import sys, os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from src.config import (\n",
    "    VEHICLE_DIR, NON_VEHICLE_DIR,\n",
    "    HOG_FEATURE_PATH, HOG_LABEL_PATH\n",
    ")\n",
    "from src.preprocessing import (\n",
    "    fetch_and_sample_image_paths,\n",
    "    load_and_extract_features,               # HOG-only\n",
    "    load_and_extract_hog_hsv_features,       # HOG+HSV\n",
    "    load_and_extract_hsv_lbp_features        # HSV+LBP\n",
    ")\n",
    "\n",
    "print(\"Ready! VEHICLE_DIR =\", VEHICLE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029e678d-54c9-4e2d-9f3f-91337de84e70",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ostan98/gitfolder/Vehicle_Detection_and_Classification/Data/vehicle'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ─── Cell 2: Collect image paths & labels ─────────────────────────\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m annotations = \u001b[43mfetch_and_sample_image_paths\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mVEHICLE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mNON_VEHICLE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✔️ Collected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(annotations)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m total samples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLabel distribution:\u001b[39m\u001b[33m\"\u001b[39m, Counter(label \u001b[38;5;28;01mfor\u001b[39;00m _, label \u001b[38;5;129;01min\u001b[39;00m annotations))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitfolder/Vehicle_Detection_and_Classification/src/preprocessing.py:26\u001b[39m, in \u001b[36mfetch_and_sample_image_paths\u001b[39m\u001b[34m(vehicle_dir, non_vehicle_dir, sample_size)\u001b[39m\n\u001b[32m     23\u001b[39m annotations = []\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Vehicle subfolders\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     27\u001b[39m     folder_path = os.path.join(vehicle_dir, folder)\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(folder_path):\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/ostan98/gitfolder/Vehicle_Detection_and_Classification/Data/vehicle'"
     ]
    }
   ],
   "source": [
    "# ─── Cell 2: Collect image paths & labels ─────────────────────────\n",
    "annotations = fetch_and_sample_image_paths(\n",
    "    VEHICLE_DIR,\n",
    "    NON_VEHICLE_DIR,\n",
    "    sample_size=300\n",
    ")\n",
    "\n",
    "print(f\"✔️ Collected {len(annotations)} total samples\")\n",
    "print(\"Label distribution:\", Counter(label for _, label in annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139adf59-b600-40ad-98d4-ddfbbc5a6a2a",
   "metadata": {},
   "source": [
    "## **HOG Feature Extraction and HSV**\n",
    "\n",
    "Extract & Save HOG-only Features (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e22554-b60f-41ee-8458-7465c492f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ HOG-only feature matrix shape: (1500, 8100)\n",
      "✅ Saved HOG-only features and scaler\n"
     ]
    }
   ],
   "source": [
    "#Extract & Save HOG-only Features (SVM) ───────────────\n",
    "X_hog, y_hog = load_and_extract_features(annotations)\n",
    "print(\"✔️ HOG-only feature matrix shape:\", X_hog.shape)\n",
    "\n",
    "scaler_hog = StandardScaler()\n",
    "X_hog_scaled = scaler_hog.fit_transform(X_hog)\n",
    "\n",
    "# np.save(\"../Data/splits/hog_features.npy\", X_hog_scaled)\n",
    "# np.save(\"../Data/splits/hog_labels.npy\", y_hog)\n",
    "joblib.dump(scaler_hog, \"../model/hog_scaler.pkl\")\n",
    "print(\"✅ Saved HOG-only features and scaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f99ea4-b4cb-42ad-a1c8-87f99e884e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting HSV+LBP features: 100%|█████████████████████████████████████████████████| 1500/1500 [18:31<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ HSV+LBP feature matrix shape: (1500, 768)\n",
      "✅ Saved HSV+LBP features and scaler\n"
     ]
    }
   ],
   "source": [
    "#Extract & Save HSV+LBP Features ──────────────────────\n",
    "X_hsv_lbp, y_hsv_lbp = load_and_extract_hsv_lbp_features(annotations)\n",
    "print(\"✔️ HSV+LBP feature matrix shape:\", X_hsv_lbp.shape)\n",
    "\n",
    "scaler_hsv_lbp = StandardScaler()\n",
    "X_hsv_lbp_scaled = scaler_hsv_lbp.fit_transform(X_hsv_lbp)\n",
    "\n",
    "np.save(\"../Data/splits/hsv_lbp_features.npy\", X_hsv_lbp_scaled)\n",
    "np.save(\"../Data/splits/hsv_lbp_labels.npy\", y_hsv_lbp)\n",
    "\n",
    "joblib.dump(scaler_hsv_lbp, \"../model/hsv_lbp_scaler.pkl\")\n",
    "print(\"✅ Saved HSV+LBP features and scaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e5bae7-6a26-487b-aa1c-3e977f9d9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting HOG+HSV features: 100%|█████████████████████████████████████████████████| 1500/1500 [02:34<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Combined feature matrix shape: (1500, 8612)\n",
      "✅ Saved combined HOG+HSV features and scaler\n"
     ]
    }
   ],
   "source": [
    "#Extract & Save HOG+HSV Combined Features ─────────────\n",
    "X_combined, y_combined = load_and_extract_hog_hsv_features(annotations)\n",
    "print(\"✔️ Combined feature matrix shape:\", X_combined.shape)\n",
    "\n",
    "scaler_combined = StandardScaler()\n",
    "X_combined_scaled = scaler_combined.fit_transform(X_combined)\n",
    "\n",
    "np.save(HOG_FEATURE_PATH, X_combined_scaled)\n",
    "np.save(HOG_LABEL_PATH, y_combined)\n",
    "joblib.dump(scaler_combined, \"../model/cfeature_scaler.pkl\")\n",
    "print(\"✅ Saved combined HOG+HSV features and scaler\")\n",
    "\n",
    "\n",
    "joblib.dump(scaler_hsv_lbp, \"../model/hsv_hog_scaler.pkl\")\n",
    "print(\"✅ Saved HSV+LBP features and scaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f93b5f2-af63-4038-a041-ba83ff25b957",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# np.save(HOG_FEATURE_PATH, X_scaled)\n",
    "# np.save(HOG_LABEL_PATH, y)\n",
    "# print(f\"✅ Saved scaled features to {HOG_FEATURE_PATH}\")\n",
    "# print(f\"✅ Saved labels to {HOG_LABEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ed858-7adb-4aff-8513-611fa25bd63e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# np.save(HOG_FEATURE_PATH, X)\n",
    "# np.save(HOG_LABEL_PATH, y)\n",
    "\n",
    "# print(f\"✅ Saved features to {HOG_FEATURE_PATH}\")\n",
    "# print(f\"✅ Saved labels   to {HOG_LABEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332afaab-9f75-4a18-8d79-0c81f60a2533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
